#!/usr/bin/env python
# =============================================================================
# Model Evaluation Script
# =============================================================================
"""
Evaluate pronunciation scoring model on test set.

Generates comprehensive evaluation metrics:
- GOP score correlation
- Phoneme accuracy
- Latency benchmarks
- Detailed error analysis

Usage:
    python evaluate.py --config configs/eval_config.yaml
"""

import argparse
import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, List
from dataclasses import dataclass, asdict

import numpy as np
import yaml

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationResult:
    """Container for evaluation results."""
    
    # Pronunciation Metrics
    gop_correlation: float
    gop_mae: float
    phoneme_accuracy: float
    word_error_rate: float
    
    # Performance Metrics
    latency_p50_ms: float
    latency_p99_ms: float
    throughput_audio_sec_per_sec: float
    
    # Sample Details
    num_samples: int
    total_audio_duration_sec: float


def load_config(config_path: Path) -> Dict[str, Any]:
    """Load evaluation configuration."""
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def calculate_gop_metrics(
    predicted_scores: np.ndarray,
    ground_truth_scores: np.ndarray,
) -> Dict[str, float]:
    """
    Calculate GOP-related metrics.
    
    GOP (Goodness of Pronunciation) measures how well a phoneme
    was pronounced compared to native speakers.
    """
    # Pearson correlation
    correlation = np.corrcoef(predicted_scores, ground_truth_scores)[0, 1]
    
    # Mean Absolute Error
    mae = np.mean(np.abs(predicted_scores - ground_truth_scores))
    
    # Root Mean Square Error
    rmse = np.sqrt(np.mean((predicted_scores - ground_truth_scores) ** 2))
    
    return {
        "correlation": correlation,
        "mae": mae,
        "rmse": rmse,
    }


def calculate_phoneme_accuracy(
    predicted_phonemes: List[List[str]],
    ground_truth_phonemes: List[List[str]],
) -> float:
    """Calculate phoneme-level accuracy."""
    total_correct = 0
    total_phonemes = 0
    
    for pred, gt in zip(predicted_phonemes, ground_truth_phonemes):
        min_len = min(len(pred), len(gt))
        total_correct += sum(p == g for p, g in zip(pred[:min_len], gt[:min_len]))
        total_phonemes += len(gt)
    
    return total_correct / total_phonemes if total_phonemes > 0 else 0.0


def calculate_word_error_rate(
    hypotheses: List[str],
    references: List[str],
) -> float:
    """
    Calculate Word Error Rate (WER).
    
    WER = (S + D + I) / N
    where S=substitutions, D=deletions, I=insertions, N=reference words
    """
    total_wer = 0.0
    total_words = 0
    
    for hyp, ref in zip(hypotheses, references):
        hyp_words = hyp.lower().split()
        ref_words = ref.lower().split()
        
        # Simple WER calculation using edit distance
        m, n = len(ref_words), len(hyp_words)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
            
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if ref_words[i-1] == hyp_words[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        total_wer += dp[m][n]
        total_words += m
    
    return total_wer / total_words if total_words > 0 else 0.0


def benchmark_latency(
    model,
    test_samples: List[Dict],
    num_warmup: int = 10,
    num_runs: int = 100,
) -> Dict[str, float]:
    """
    Benchmark model inference latency.
    
    Returns:
        Dictionary with p50, p90, p99 latencies in milliseconds
    """
    latencies = []
    
    # Warmup runs
    for i in range(min(num_warmup, len(test_samples))):
        _ = model(test_samples[i])
    
    # Timed runs
    for i in range(min(num_runs, len(test_samples))):
        start = time.perf_counter()
        _ = model(test_samples[i])
        end = time.perf_counter()
        latencies.append((end - start) * 1000)  # Convert to ms
    
    latencies = np.array(latencies)
    
    return {
        "p50": np.percentile(latencies, 50),
        "p90": np.percentile(latencies, 90),
        "p99": np.percentile(latencies, 99),
        "mean": np.mean(latencies),
        "std": np.std(latencies),
    }


def generate_error_analysis(
    predictions: List[Dict],
    ground_truths: List[Dict],
) -> Dict[str, Any]:
    """
    Generate detailed error analysis.
    
    Identifies:
    - Most commonly mispronounced phonemes
    - Vietnamese-specific error patterns
    - Per-speaker analysis
    """
    error_patterns = {
        "th_errors": 0,  # th â†’ t/s
        "r_errors": 0,   # r â†’ l
        "final_consonant_errors": 0,  # Dropping final consonants
        "vowel_length_errors": 0,  # Short/long vowel confusion
    }
    
    phoneme_errors = {}
    
    for pred, gt in zip(predictions, ground_truths):
        # Analyze phoneme-level errors
        pred_phonemes = pred.get("phonemes", [])
        gt_phonemes = gt.get("phonemes", [])
        
        for p, g in zip(pred_phonemes, gt_phonemes):
            if p != g:
                error_key = f"{g}->{p}"
                phoneme_errors[error_key] = phoneme_errors.get(error_key, 0) + 1
    
    # Sort by frequency
    top_errors = sorted(phoneme_errors.items(), key=lambda x: -x[1])[:10]
    
    return {
        "vietnamese_specific": error_patterns,
        "top_phoneme_errors": dict(top_errors),
    }


def generate_report(
    results: EvaluationResult,
    error_analysis: Dict[str, Any],
    output_path: Path,
) -> None:
    """Generate HTML evaluation report."""
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>VietSpeak AI - Evaluation Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            h1 {{ color: #333; }}
            .metric {{ margin: 20px 0; padding: 15px; background: #f5f5f5; border-radius: 8px; }}
            .metric-value {{ font-size: 24px; font-weight: bold; color: #2196F3; }}
            .section {{ margin: 30px 0; }}
            table {{ border-collapse: collapse; width: 100%; }}
            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
            th {{ background: #333; color: white; }}
        </style>
    </head>
    <body>
        <h1>ðŸŽ¤ VietSpeak AI - Model Evaluation Report</h1>
        
        <div class="section">
            <h2>Pronunciation Metrics</h2>
            <div class="metric">
                <div>GOP Correlation</div>
                <div class="metric-value">{results.gop_correlation:.4f}</div>
            </div>
            <div class="metric">
                <div>Phoneme Accuracy</div>
                <div class="metric-value">{results.phoneme_accuracy*100:.2f}%</div>
            </div>
            <div class="metric">
                <div>Word Error Rate</div>
                <div class="metric-value">{results.word_error_rate*100:.2f}%</div>
            </div>
        </div>
        
        <div class="section">
            <h2>Performance Metrics</h2>
            <div class="metric">
                <div>Latency (P50)</div>
                <div class="metric-value">{results.latency_p50_ms:.2f} ms</div>
            </div>
            <div class="metric">
                <div>Latency (P99)</div>
                <div class="metric-value">{results.latency_p99_ms:.2f} ms</div>
            </div>
            <div class="metric">
                <div>Throughput</div>
                <div class="metric-value">{results.throughput_audio_sec_per_sec:.2f}x realtime</div>
            </div>
        </div>
        
        <div class="section">
            <h2>Dataset Statistics</h2>
            <p>Samples: {results.num_samples}</p>
            <p>Total Audio: {results.total_audio_duration_sec:.2f} seconds</p>
        </div>
    </body>
    </html>
    """
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w") as f:
        f.write(html_content)
    
    logger.info(f"Report saved to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Evaluate pronunciation model")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("configs/eval_config.yaml"),
        help="Path to evaluation configuration",
    )
    
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    logger.info(f"Evaluating: {config['experiment']['name']}")
    
    # Placeholder evaluation results (replace with actual model inference)
    results = EvaluationResult(
        gop_correlation=0.85,
        gop_mae=0.12,
        phoneme_accuracy=0.78,
        word_error_rate=0.15,
        latency_p50_ms=45.0,
        latency_p99_ms=120.0,
        throughput_audio_sec_per_sec=35.0,
        num_samples=1000,
        total_audio_duration_sec=3600.0,
    )
    
    error_analysis = {
        "vietnamese_specific": {
            "th_errors": 150,
            "r_errors": 120,
            "final_consonant_errors": 200,
        },
        "top_phoneme_errors": {
            "Î¸->t": 50,
            "Ã°->d": 45,
            "r->l": 40,
        },
    }
    
    # Save results
    results_dir = Path(config["output"]["results_dir"])
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # Save JSON results
    with open(results_dir / "metrics.json", "w") as f:
        json.dump(asdict(results), f, indent=2)
    
    # Generate HTML report
    if config["output"]["generate_report"]:
        generate_report(
            results,
            error_analysis,
            results_dir / "report.html",
        )
    
    logger.info("Evaluation complete!")
    logger.info(f"Results saved to {results_dir}")


if __name__ == "__main__":
    main()
